# Shannon Text Generation: From Information Theory to Modern AI

![Python Version](https://img.shields.io/badge/python-3.7+-blue.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)

## Overview

This project implements Claude Shannon's foundational work on information theory and statistical language modeling. It demonstrates how **n-gram Markov chains** can approximate natural language at progressively more sophisticated levels, from random character sequences to nearly coherent prose.

**Key Features:**
- ðŸ“Š **7 Approximation Levels**: From char-0 (random) to word-3 (trigram-based)
- ðŸ” **Text Analysis Pipeline**: Extract character and word n-gram frequencies
- ðŸ“ **Text Generation Engine**: Generate text using Markov chains
- ðŸŽ¯ **Anchor Words**: Integrate specific words naturally into generated text
- ðŸŽ­ **Multi-Author Support**: Generate text mimicking Austen, Twain, or Doyle
- ðŸ”€ **Style Blending**: (Bonus) Mix styles from multiple authors
- ðŸ’» **CLI Interface**: Full command-line tool for all operations

## Project Structure

```
Shannon_to_Modern_AI/
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ starter_preprocess.py              # Text preprocessing utilities (provided)
â”œâ”€â”€ analyze.py                         # Part 2: Statistical analysis
â”œâ”€â”€ generator.py                       # Part 3: Text generation engine
â”œâ”€â”€ shannon_gen.py                     # Part 4: CLI interface
â”œâ”€â”€ test_generator.py                  # Testing suite
â”‚
â”œâ”€â”€ Text Files:
â”œâ”€â”€ austen_pride_prejudice.txt         # Source text: Pride and Prejudice
â”œâ”€â”€ twain_tom_sawyer.txt               # Source text: Tom Sawyer
â”œâ”€â”€ doyle_sherlock_holmes.txt          # Source text: Sherlock Holmes
â”‚
â”œâ”€â”€ JSON Frequency Files (generated by analyze.py):
â”œâ”€â”€ austen_*.json                      # Austen frequency tables
â”œâ”€â”€ twain_*.json                       # Twain frequency tables
â”œâ”€â”€ doyle_*.json                       # Doyle frequency tables
â”‚
â””â”€â”€ Analysis:
    â””â”€â”€ Part5_Analysis_Report.md       # Assignment 5: Analysis and Reflection
```

## Installation

### Prerequisites

- **Python 3.7+**
- **macOS/Linux/Windows** with terminal access

### Setup Instructions

1. **Clone or download the project:**
   ```bash
   cd Shannon_to_Modern_AI
   ```

2. **Verify all required files are present:**
   ```bash
   # Check that all Python files exist
   ls *.py
   
   # Check that all text files exist
   ls *.txt
   ```

3. **Install dependencies (if needed):**
   The project uses only Python standard library. No external packages required!
   ```bash
   # Just Python 3.7+ is needed
   python3 --version
   ```

4. **Verify the setup:**
   ```bash
   # Test that modules import correctly
   python3 -c "from starter_preprocess import TextPreprocessor; print('âœ… Setup successful!')"
   ```

## Quick Start

### Option 1: Generate Text Immediately (if JSON files exist)

If you already have the frequency JSON files (from running `analyze.py`):

```bash
# Generate text at word-3 (highest quality) for Austen
python3 shannon_gen.py generate --author austen --level word-3 --sentences 3

# Generate from all three authors
python3 shannon_gen.py compare --author austen --sentences 2
```

### Option 2: Run Full Pipeline (from scratch)

```bash
# Step 1: Analyze texts and generate frequency tables
python3 shannon_gen.py analyze --author austen --file austen_pride_prejudice.txt
python3 shannon_gen.py analyze --author twain --file twain_tom_sawyer.txt
python3 shannon_gen.py analyze --author doyle --file doyle_sherlock_holmes.txt

# Step 2: Generate text
python3 shannon_gen.py generate --author austen --level word-3 --sentences 5

# Step 3: Compare all approximation levels
python3 shannon_gen.py compare --author austen --sentences 2

# Step 4: Test with anchor words
python3 shannon_gen.py generate --author austen --level word-2 --sentences 3 --anchors elizabeth,bennet,pride
```

### Option 3: Run Tests

```bash
# Test the generator to ensure everything works
python3 test_generator.py
```

## Usage Guide

### Command: `analyze`

Extract frequency tables from text files. Generates 7 JSON files per author.

```bash
python3 shannon_gen.py analyze --author AUTHOR --file FILEPATH
```

**Arguments:**
- `--author` (required): `austen`, `twain`, or `doyle`
- `--file` (required): Path to text file (e.g., `austen_pride_prejudice.txt`)

**Example:**
```bash
python3 shannon_gen.py analyze --author austen --file austen_pride_prejudice.txt
```

**Output:**
- `austen_char_unigrams.json`
- `austen_char_bigrams.json`
- `austen_char_trigrams.json`
- `austen_word_unigrams.json`
- `austen_word_bigrams.json`
- `austen_word_trigrams.json`
- `austen_sentence_stats.json`

---

### Command: `generate`

Generate text at a specific approximation level.

```bash
python3 shannon_gen.py generate --author AUTHOR --level LEVEL [--length LENGTH] [--sentences SENTENCES] [--anchors WORDS]
```

**Arguments:**
- `--author` (required): `austen`, `twain`, or `doyle`
- `--level` (required): Approximation level
  - `char-0`: Random characters (equal probability)
  - `char-1`: English character frequencies
  - `char-2`: Character bigrams (2nd-order Markov)
  - `char-3`: Character trigrams (3rd-order Markov)
  - `word-1`: Word unigrams (random words)
  - `word-2`: Word bigrams (2nd-order Markov)
  - `word-3`: Word trigrams (3rd-order Markov)
- `--length` (optional): Character count for char-level (default: 100)
- `--sentences` (optional): Sentence count for word-level (default: 3)
- `--anchors` (optional): Comma-separated words to include (no spaces)

**Examples:**

```bash
# Generate 100 characters at char-2 level
python3 shannon_gen.py generate --author austen --level char-2 --length 100

# Generate 5 sentences at word-3 level
python3 shannon_gen.py generate --author twain --level word-3 --sentences 5

# Generate with anchor words
python3 shannon_gen.py generate --author austen --level word-2 --sentences 3 --anchors elizabeth,bennet,pride

# Generate from Doyle with mystery-related anchors
python3 shannon_gen.py generate --author doyle --level word-3 --sentences 4 --anchors elementary,Watson,deduce
```

**Output Examples:**

**char-0** (pure random):
```
XFOML RXKHRJFFJUJ ZLPWCFWK CYJNA
```

**word-3** (highest quality):
```
Elizabeth could not help observing as she turned over some books on the table.
He was very much pleased with her manner and conversation.
```

---

### Command: `compare`

Compare all 7 approximation levels for a single author side-by-side.

```bash
python3 shannon_gen.py compare --author AUTHOR [--sentences SENTENCES]
```

**Arguments:**
- `--author` (required): `austen`, `twain`, or `doyle`
- `--sentences` (optional): Sentences per level for word-level (default: 2)

**Example:**
```bash
python3 shannon_gen.py compare --author austen --sentences 2
```

**Output:**
Displays all 7 levels (char-0 through word-3) with examples for each, allowing you to see the progression of quality.

---

### Command: `blend` (Bonus)

Blend text generation styles from multiple authors.

```bash
python3 shannon_gen.py blend --authors AUTHOR1,AUTHOR2 --level LEVEL [--sentences SENTENCES]
```

**Arguments:**
- `--authors` (required): Comma-separated author names (e.g., `austen,twain`)
- `--level` (required): Approximation level (`char-0` through `word-3`)
- `--sentences` (optional): Sentences per author (default: 3)

**Example:**
```bash
# Blend Austen and Twain at word-2 level
python3 shannon_gen.py blend --authors austen,twain --level word-2 --sentences 3

# Blend all three authors
python3 shannon_gen.py blend --authors austen,twain,doyle --level word-3 --sentences 2
```

---

## Understanding the Approximation Levels

The 7 levels represent increasing sophistication in modeling language structure:

| Level | Model | Description | Output Quality |
|-------|-------|-------------|-----------------|
| **char-0** | Uniform | Random characters | Gibberish |
| **char-1** | Unigram | English letter frequencies | Slightly better gibberish |
| **char-2** | Bigram | 2-character sequences | Word fragments emerge |
| **char-3** | Trigram | 3-character sequences | Recognizable English words |
| **word-1** | Unigram | Random valid words | Word salad |
| **word-2** | Bigram | Word pairs (Markov) | Grammatical phrases |
| **word-3** | Trigram | Word triplets (Markov) | Nearly coherent sentences |

**Key insight from Shannon:** Even without any explicit grammar rules or semantic understanding, progressively higher-order statistical models produce increasingly human-like language. This demonstrates that language has deep statistical structure.

---

## Testing and Validation

### Automated Testing

```bash
# Run comprehensive test suite
python3 test_generator.py
```

This runs 8 tests:
1. âœ… JSON files exist
2. âœ… Generator imports
3. âœ… Generator initialization
4. âœ… Character-level generation (all 4 levels)
5. âœ… Word-level generation (all 3 levels)
6. âœ… Anchor word integration
7. âœ… All three authors
8. âœ… Sentence length distribution

### Manual Testing

```bash
# Test single command
python3 shannon_gen.py generate --author austen --level word-3 --sentences 1

# Save output to file for review
python3 shannon_gen.py compare --author austen --sentences 2 > output.txt
```

---

## Project Architecture

### 1. `starter_preprocess.py` (Provided)

Handles text preprocessing:
- **TextPreprocessor**: Cleans text, removes Gutenberg headers, tokenizes
- **FrequencyAnalyzer**: Computes n-grams and their frequencies

### 2. `analyze.py` (Part 2)

Extracts statistical information from texts:
- Loads and cleans text using `TextPreprocessor`
- Computes character and word n-grams (unigrams, bigrams, trigrams)
- Analyzes sentence structure (lengths, distributions)
- Saves all statistics as JSON files

### 3. `generator.py` (Part 3)

Text generation using n-gram Markov chains:
- **TextGenerator class**: Loads JSON frequency files
- Implements 7 generation methods (char-0 through word-3)
- Converts frequencies to probability distributions
- Builds Markov chains from n-grams
- Samples from chains to generate text
- Integrates anchor words naturally

### 4. `shannon_gen.py` (Part 4)

Command-line interface:
- Uses `argparse` for CLI argument parsing
- Dispatches to four main commands: analyze, generate, compare, blend
- Handles user input validation and error reporting

---

## Data Flow

```
Text File (*.txt)
      â†“
    [analyze.py]
      â†“
   JSON Files (*.json)
      â†“
  [generator.py]
      â†“
  TextGenerator Class
      â†“
  [shannon_gen.py CLI]
      â†“
  Generated Text Output
```

---

## Example Workflow

### Complete Analysis â†’ Generation â†’ Comparison Workflow

```bash
# 1. Start fresh: analyze all three texts
python3 shannon_gen.py analyze --author austen --file austen_pride_prejudice.txt
python3 shannon_gen.py analyze --author twain --file twain_tom_sawyer.txt
python3 shannon_gen.py analyze --author doyle --file doyle_sherlock_holmes.txt

# 2. Test individual generations
echo "--- Testing Austen ---"
python3 shannon_gen.py generate --author austen --level word-3 --sentences 2

echo "--- Testing Twain ---"
python3 shannon_gen.py generate --author twain --level word-3 --sentences 2

echo "--- Testing Doyle ---"
python3 shannon_gen.py generate --author doyle --level word-3 --sentences 2

# 3. Compare all levels for quality progression
python3 shannon_gen.py compare --author austen --sentences 1

# 4. Test advanced features
python3 shannon_gen.py generate --author austen --level word-2 --sentences 3 --anchors elizabeth,bennet,pride

# 5. Blend multiple authors
python3 shannon_gen.py blend --authors austen,twain --level word-2 --sentences 3

# 6. Run tests to validate everything
python3 test_generator.py
```

---

## Troubleshooting

### Issue: `ModuleNotFoundError: No module named 'starter_preprocess'`

**Solution:** Make sure `starter_preprocess.py` is in the same directory as the other Python files.

```bash
# Check if file exists
ls starter_preprocess.py

# If missing, ensure it's downloaded from the assignment
```

### Issue: `FileNotFoundError: austen_word_unigrams.json not found`

**Solution:** Run `analyze.py` first to generate the JSON frequency files.

```bash
python3 shannon_gen.py analyze --author austen --file austen_pride_prejudice.txt
```

### Issue: Text generation produces empty output or errors

**Solution:** Verify JSON files contain valid data:

```bash
# Check file size (should be > 1KB)
ls -lh austen_*.json

# Try re-analyzing the text
python3 shannon_gen.py analyze --author austen --file austen_pride_prejudice.txt
```

### Issue: `FileNotFoundError: austen_pride_prejudice.txt not found`

**Solution:** Ensure text files are in the project directory:

```bash
# List available text files
ls *.txt
```

### Issue: Command not recognized

**Solution:** Make sure you're using the correct syntax:

```bash
# CORRECT:
python3 shannon_gen.py generate --author austen --level word-3

# WRONG (missing required arguments):
python3 shannon_gen.py generate
```

---

## Key Concepts

### Shannon's Information Theory

Claude Shannon demonstrated that English text exhibits statistical regularities at multiple levels. By modeling language as sequences of probabilistic events, we can approximate natural language without explicit grammar rules.

### Markov Chains

The core technique is the **Markov chain**: a model where the next state depends only on the current state. At word-2 level, this means: given the current word, what word is likely to come next?

### n-grams

An **n-gram** is a sequence of n tokens. Examples:
- Unigram (1-gram): "the"
- Bigram (2-gram): "the quick"
- Trigram (3-gram): "the quick brown"

By storing frequencies of n-grams from the source text, we can reconstruct plausible sequences.

---

## Performance Expectations

- **Text Analysis**: ~5-15 seconds per author (depending on text size)
- **Text Generation**: Instant (< 1 second per generation)
- **Memory Usage**: ~10-50 MB (depending on frequency table sizes)
- **JSON File Size**: ~1-5 MB per author

---

## Limitations and Future Work

### Current Limitations

- **Limited context**: Word-3 can only look back 2 words (fixed window)
- **No long-range coherence**: Generated text loses coherence over many sentences
- **No semantic understanding**: Model has no understanding of meaning
- **Anchor words**: Forced anchor integration can degrade quality

### Future Improvements

- **Neural networks**: Implement transformer-based models (attention mechanisms)
- **Longer context**: Use bidirectional models that attend to relevant context anywhere
- **Semantic embeddings**: Learn dense vector representations of words
- **Fine-tuning**: Adapt models to specific domains or authors
- **Beam search**: Generate multiple candidates and rank by quality

---

## Assignment Structure

This project implements Assignment 3 for CSE 510: Basics of AI.

- **Part 1**: Background (Shannon's theory)
- **Part 2**: `analyze.py` - Statistical analysis
- **Part 3**: `generator.py` - Text generation engine
- **Part 4**: `shannon_gen.py` - CLI interface
- **Part 5**: `Part5_Analysis_Report.md` - Analysis and reflection

---

## File Checklist

Before submission, ensure you have:

```
âœ… analyze.py                          (Part 2)
âœ… generator.py                        (Part 3)
âœ… shannon_gen.py                      (Part 4)
âœ… test_generator.py                   (Testing)
âœ… starter_preprocess.py               (Provided)
âœ… Part5_Analysis_Report.md            (Part 5)
âœ… README.md                           (This file)
âœ… All JSON frequency files            (21 files: 7 per author)
âœ… All text files                      (3 files: Austen, Twain, Doyle)
âœ… requirements.txt                    (Optional, lists dependencies)
```

---

## Requirements

Since this project uses only Python standard library:

```
python>=3.7
```

Optional (for development):
```
pytest>=6.0  (for extended testing)
black>=21.0  (for code formatting)
pylint>=2.0  (for linting)
```

---

## License

This project is for educational purposes as part of CSE 510: Basics of AI.

---

## References

- **Shannon, C. E.** (1948). "A Mathematical Theory of Communication." *The Bell System Technical Journal*, 27(3), 379-423.
- Assignment 3: Approximating Natural Language - From Shannon to Modern AI

---

## Questions or Issues?

For problems or questions:

1. Check the **Troubleshooting** section above
2. Run `test_generator.py` to validate setup
3. Review generated output for clues
4. Verify all files are present and in correct directory

---

## Author Notes

This implementation demonstrates how statistical language models work at a fundamental level. While simple compared to modern neural networks, n-gram models reveal the deep structure of language that Shannon first discovered in 1948.

The progression from char-0 (random gibberish) through word-3 (nearly coherent sentences) shows that language is highly predictable from local contextâ€”a key insight that motivates all modern NLP approaches.

Happy exploring! ðŸš€
